\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\usepackage[myheadings]{fullpage}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{sectsty}
\usepackage{appendix}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage[
    backend=bibtex,
    style=numeric,
    sorting=ynt
]{biblatex}
\addbibresource{bibliography.bib}

\onehalfspacing

\pagestyle{fancy}
\fancyhf{}
\setlength\headheight{15pt}
\fancyhead[L]{Student ID: 21963144}
\fancyhead[R]{UWA}
\fancyfoot[R]{Page \thepage{} of \pageref{LastPage}}
\sectionfont{\scshape}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}

\title{\normalsize \textsc{GENG5551 Research Project}
        \\ [1.5cm]
        \HRule{0.5pt} \\
        \LARGE \textbf{\uppercase{Research Proposal}}
        \HRule{2pt} \\ [0.5cm]
        \normalsize \today \vspace*{3\baselineskip}}

\author{Thomas Hill Almeida (21963144)}
\date{}

\begin{document}
\maketitle{}
\tableofcontents{}
\newpage{}

\section{Introduction}
\subsection{Background}

In 1687, Isaac Newton published his book, ``\textit{Philosophi\ae{} Naturalis Principia
Mathematica}'', containing his law of universal gravitation:

\begin{equation}
    F = G\dfrac{m_1m_2}{r^2}
\end{equation}

where \(F\) is the resultant gravitational force, \(G\) is the universal gravitational constant,
\(m\) is the mass of the objects and \(r\) is the distance between the objects.

It has since been taught in high-schools across the world in introductory physics classes with the
formula , due in part to its simplicity and general accuracy for predicting movement. Despite its
general applicability, one major issue of the theory is that it infers that gravitational is
instantaneously applied, without any apparent method through which it could be transmitted.

Roughly two centuries later, in 1905, Albert Einstein presented the theory of special relativity
in his paper ``\textit{Zur Elektrodynamik bewegter K{\"o}rper}'' (English: \textit{``On the
Electrodynamics of Moving Bodies''}). The theory introduced the concept of \textbf{spacetime} to
describe inertial reference frames as a four-dimensional coordinate system, \((t, x, y, z)\), where
$t$ is time and $(x, y, z)$ are the three spatial dimensions. He further stated two important
axioms; that the speed of light in a vacuum is the same for all observers regardless of motion and
that the laws of physics are invariant in all inertial frames of reference. About ten years later,
Albert Einstein incorporated the effect of gravity with special relativity, forming the general
theory of relativity.

The general theory of relativity postulates that the effect of gravity can be characterised as each
gravitational potential source changing the curvature of spacetime. The relationship of
gravitational mass-energy and the shape of spacetime is given by Einstein's field equations:

\begin{equation}
    G_{\mu{}v} + \Lambda{}g_{\mu{}v} = \dfrac{8\pi{}G}{c^4}T_{\mu{}v}
\end{equation}

where \(T_{\mu{}v}\) is the stress-energy tensor \footnote{Tensors are very similar to matrices of
vectors and are typically used to describe mathematical geometric relationships}, \(G_{\mu{}v}\) is
the Einstein tensor, \(g_{\mu{}v}\) is the spacetime metric, \(\Lambda\) is the cosmological
constant, \(G\) is the universal gravitational constant and \(c\) is the speed of light.

An implication of gravity curving spacetime is that massive accelerating objects would cause
`ripples' in fabric of spacetime called gravitational waves. The existence of gravitational waves
remained a theory until 1974, when Russell Hulse and Joseph Taylor discovered a binary pair of
neutron stars that were orbiting each other. After several years of measurement, they found that
the speed at which the stars were orbiting each other was slowing in a manner consistent with the
predictions of the general theory of relativity, showing that gravitational waves did indeed exist.

There were several experiments performed in the 1960s and 1970s to determine methods to detect
gravitaional waves, resulting in several large laser interferometric detectors (that is, detectors
that use interferometry \textendash{} the phenomena by which wave superpose on each other to create
a resultant wave \textendash{} for detection) being built throughout the early 2000s, including the
American Laser Interferometric Gravitational-Wave Observatory (LIGO), the Italian Virgo detector,
and the German GEO600 detector. The initial observation runs between 2002 and 2011 by the various
detectors failed to directly detect any gravitational waves, and as such, the majority of the
detectors began work to increase their sensitivity throughout the 2010s. The increase in detector
sensitivity has brought success in the search for gravitational waves, with the first direct
detection occurring on the 14th of September, 2015.

Due to their design, the detectors have a significant amount of noise from sources that are not
gravitational waves, in addition to the gravitational waves themselves having very weak signals. As
such, a large amount of data processing needs to be done to the outputs producted by the detectors
in order to filter and extract any possible gravitational waves. These data processors are known as
`\textit{pipelines}', and are mostly created by research groups that are a part of the LIGO
Scientific Collaboration. These pipelines are used throughout observation runs for real-time data
analysis.

One such pipeline is the Summed Parallel Infinite Impulse Response (SPIIR) pipeline, created by
Shaun Hooper in 2012. The pipeline uses a number of IIR filters \textendash{} which are commonly
used in signal processing for bandpass filtering \textendash{} to approximate possible gravitational
wave signals for detection. The pipeline was further developed by Qi Chu in 2017, by using GPU
acceleration techniques to increase the speed of analysis, as well implementing a method to use a
frequentist coherent search. The pipeline is currently the fastest of all existing pipelines, and
has participated in every observation run since November 2015, successfully detecting all events
that were seen in more than one detector.

The SPIIR pipeline uses GStreamer, a library for composing, filtering and moving around signals, in
addition to the GStreamer LIGO Algorithm Library (\texttt{gstlal}). After receiving data from the
detectors, the pipeline performs data conditioning and data whitening, followed by the usage of the
IIR filters. The data is then combined for post-processing, where events are given sky localization
and then inserted into the LIGO event database.

\subsection{Problem Description \& Goal}
As of \today{}, the SPIIR pipeline supports the use of two or three detectors for gravitational
wave searching \textendash{} the two American LIGO detectors and the Virgo detector. There are
several issues with the current pipeline design that this research project aims to address.

Further detectors are likely to be coming online in the near future, with old detectors occasionally
being removed from detection for maintainance. For example, the Japanese KAGRA detector is
undergoing testing with the goal of being used in the next observation run, and LIGO India is
currently being installed. With the current design of the pipeline, adding and removing detectors is
a significant undertaking that takes a substantial amount of development time.

In addition, if a detector is indeed added to SPIIR, the detector \textit{\textbf{must}} be used for
the coherent post-processing, and can't be used just for synchronization purposes. This presents an
issue as additional detectors are added, as each detector has its own sensitivity, reading
variations and range of observable gravitational wave frequencies, resulting in some detectors being
suitable for searching specific frequency ranges whilst showing no discernable change in output for
other detectors, causing many false negatives.

An ideal architecture for the pipeline would be significantly more composable, able to add and
remove the usage of different detectors for post-processing with minimal effort.
\\

As such, this research project aims to complete a subsection of this idealised architecture. The
project aims to remove the requirement for all detectors to be used for coherent post-processing
with sky localization, and instead aims to provide a generic interface that would allow for any
number of detectors to be used for coherent post-processing, whilst still allowing the unused
detectors to undergo all other parts of the pipeline and remain synchronized with the used
detectors. The project shall explore a number of different possible codebase refactorings as well as
exploring new techniques for efficiently combining \(N\) data sources for coherent search, and shall
measure the performance impact of the changes using a number of benchmarks.

Should time allow, this project also aims to make the internal SPIIR interface entirely generic,
removing the requirement for specific detectors to be used throughout the pipeline with specialized
code. If this aim is met, the required effort for the addition or remove of detectors would be
significantly lowered.

\section{Literature Review}

This research project aims to refactor the SPIIR pipeline codebase and explore techniques for
combining some unknown \(N\) number of data sources for coherent search. The literature for both
parts of the research are developing and varied.

\\

Refactoring is defined by \cite{Murphy} as the process of changing the structure of software without
changing its behaviour. Of course, this isn't the only way to modify source code to address known
issues. George Fairbanks offers the options of ignoring, refactoring or rewriting code as potential
methods to deal with problems in \cite{Fairbanks}, and makes several distinctions between the
options. According to Fairbanks, the major difference between refactoring and rewriting is the
process by which the code is modified. When refactoring, incremental changes are made to the
odebase, with the major goal being to keep the newly written code integrated with the existing
codebase and tests, as the outputs of a module given some inputs should still remain identical. In
contrast, when rewriting, the new code is written using none of the existing codebase, possibly
resulting in majorly different outputs and possibly even data flow architecture. Unlike with
refactoring, existing tests might not be able to be leveraged, but it becomes much easier to make
sweeping architectural changes to the codebase.

This still leaves the third option \textendash{} ignoring the issues. Fairbanks points out that
ignoring issues simply means that they will have to be dealt with at a later date, and contribute to
`\textit{techinical debt}' \textendash{} a term coined by Ward Cunningham in \cite{Cunningham} to
help explain why otherwise working code may need to be refactored or rewritten, and has since turned
into its own area of academic research, as well as a major focus of industry. Some examples of
activities that ``accrue'' technical debt are; a lack of documentation, implementing sub-optimal
algorithms and a lack of testing. \cite{Allman} notes that technical debt has a number of
similarities to financial debt, in that there can be advantages and disadvantages to accruing the
debt. One such advantage, is that the codebase can be shipped without being entirely complete, and
may indeed reach functional completeness in a shorter time than if the technical debt was not
accrued. Some potential disadvantages, however, include faults in the system, increased maintainance
and extensibility effort, as well as increased time onboarding new members of staff.

Technical debt, then, is clearly an area that needs to be managed over the course of a programming
project, even when rewriting or refactoring. \cite{Fairley} notes that whilst rewriting or reworking
an existing codebase can reduce or eliminate technical debt, the project also risks accumulating
additional debt if not correctly managed. To help with management, \cite{Fairley} suggests adopting
a development process that includes regularly reviewing expected and actual progress, whilst
\cite{Allman} encourages regular internal and external documentation, as well as maintaining an
index of prioritised debts.

\section{Methods}
\section{Proposed timeline}
\section{Bibliography}
\printbibliography{}
\end{document}
